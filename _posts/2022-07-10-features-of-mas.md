---
layout: post
title: "Characteristics of MAS (Multi-Agent Systems)"
categories: [Application, robots, classification]
image: assets/images/features_mas.jpg
author: ricardo
---
MAS characteristics
An agent could refer to different components as applications have different objectives and lay down in different paradigms . One can see an agent as being part of a program (e.g. model, system, or subsystem) or any type of independent entity (e.g. organization or individuals). Each agent is programmed to react to other agents and to its computational environment, with respect to behaviour rules from primitive reaction decisions to complex adaptive AI. However, one may believe that the majority of researchers should be in general agreement with Wooldridge and Jennings who defined an agent as a piece of hardware or a software-based computer system that entail the following properties:

* **Reactivity**, in the sense agents have the perception of their environment and respond quickly to changes that may occur.

* **Pro-activity**, not being limited to acting in response to the environment, agents are able to take the initiative and show behaviour driven by objectives.

* **Social skills**. The agents are able to interact/communicate (cognitive model) with other agents (and possibly humans) through a given Agent Communication Language (ACL) and establishing connections between their autonomous objectives and the spatial context.

These properties are somewhat difficult to identify than it may seem at first sight. Autonomy, although consensual in the agents’ community and essential in Wooldridge and Jennings agent definition, can never be fully achieved. It is clear that the agent has to be created and put into operation by a human (or another agent). The assumption that the agent action will not have an end is also not entirely valid. Of course, under the aegis of current science, the agent will have a limited lifetime and a final action. On the other hand, although autonomy (i.e., actions are carried out without human interference) is essential to the agent, usually human-agent interaction is desirable or even essential. It is usual to build agents that behave autonomously but are also able to take orders or instructions from humans.

To build purely reactive agents can be a simple task, but is not entirely desirable. A purely reactive agent would react to changes in environment consecutively without seeking to achieve its medium or long-term objectives, i.e. display goal-oriented behaviour. One should define agents capable of balancing reactive with proactive behaviour. Nonetheless, the difficulty in balancing these two types of behaviour is very high .

The pro-activeness is simple to get in functional systems. Still, this simplicity only applies if a static environment is considered, i.e. it does not change during the accomplishment of a given procedure or function. In addition, the agent should have all the information it needs to run that procedure or function, without any uncertainty in the environment. However, these assumptions are not valid for most environments. For dynamic and not fully accessible environments, the agents must be able to react to changes in the environment and reason if the original objectives are still valid, due to the changes in the environment while performing a given procedure. This means that the agents have to be reactive and therefore able to quickly adapt to phase-shifts in the environment.

The social capacity of an agent is related to its ability to exchange high-level messages (and not only data-bytes without an associated meaning) and carry out processes of social interaction with other agents (and/or humans) similar to those used by humans in their daily lives, establishing collective behaviours. These processes include the coordination, cooperation, and negotiation. In order to conduct them, it is necessary to reason about the objectives of the other agents (if any) present in the environment or, at least, have notion of their existence.

It is also necessary to understand that they are also autonomous agents and do not necessarily share common goals. In this way, it may be necessary to negotiate and cooperate with other agents, eventually exchanging information and/or goods. For example, in order to convince an agent to cooperate, it might be necessary to make a payment or offer a particular good or service. In several cases, agents have opposite objectives and, therefore, are not able to carry out any cooperative process that includes them.

The equilibrium between social capacity and proactive or reactive capabilities is also of great importance. This importance is even greater in a scheme of cooperative work set by a group of agents who share a common goal. In these situations, each agent has to adapt his reaction to the events that occur in the modeling environment, both with the free will needed to perform individual tasks and with the social behaviors necessary to perform collective tasks.

Some researchers highlight other aspects of agency (e.g. mobility and adaptability). Indeed, agents may have supplementary features, and in specific uses, some features can be more significant than others. Yet, is the conjugation of the three main properties (reactivity, pro-activity and social skills) in a single entity that gives importance to the agent paradigm and makes the difference between agent systems and related software paradigms, e.g. object-oriented systems, distributed systems, and expert systems.

Franklin and Graesser  discuss about various definitions of agents and list some behaviours displayed by them: 1. reactive 
2. autonomous
3. guided by objective(s)/pro-active
4. temporally continuous
5. social/communicative
6. have ability to learn/adapt
7. mobile
8. flexible
9. have personality. 
    
There are still other agents classification schemes based, for example, on the type of task they are running or in their architecture.

The very definition of agent satisfies only the first four identified features, as Franklin and Graesser  state ‘an agent is a system located in an environment and is part of it. An agent understands the environment and acts on it over time. An agent has its own agenda in order to reflect its perceptions of future’. But even so, this definition is generic enough to cover from a thermostat, containing one or two sensors and extremely simple control structure, to human beings with multiple and conflicting guidelines, various sensors, various possibilities of actions, and structures of extremely complex and sophisticated control.

The concept of agent is related to rationality. According to Russel and Norvig, rationality is associated with four factors: (i) the performance measure that defines success criteria; (ii) the prior knowledge of the agent about the environment; (iii) the actions that the agent is capable to perform; and (iv) the sequence of agent’s perceptions. These lead to the definition that ‘for each possible perceptions, a rational agent must select an action that is expected to maximize its performance measure, given the evidence provided by the sequence of perceptions and any prior internal knowledge of the agent’.

Source: [Introductory Chapter: Multi-Agent Systems](https://www.intechopen.com/chapters/56573 "Introductory Chapter: Multi-Agent Systems")